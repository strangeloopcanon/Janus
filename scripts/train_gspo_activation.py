#!/usr/bin/env python
"""Train a policy with GSPO on activation-conditioned rewrite data (no LoRA).

Input data: JSONL from `scripts/generate_rewrite_groups.py`, one row per
sample with fields: group_id, prompt, response, old_logp, advantage, etc.

This script performs sequence-level GSPO with a KL penalty to a reference
model. It updates the base model parameters directly (full-model training).
"""

from __future__ import annotations

# Ensure repo root is on sys.path when running as `python scripts/...`
import os
import sys

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import argparse
import json
from pathlib import Path
from typing import List

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer

from persona_steering_library.rl.gspo import (
    batched_sequence_logprob,
    gspo_sequence_loss,
    approx_sequence_kl,
)


class RewriteGroupDataset(Dataset):
    def __init__(self, path: Path):
        self.rows: List[dict] = []
        with path.open("r", encoding="utf-8") as fp:
            for line in fp:
                if not line.strip():
                    continue
                self.rows.append(json.loads(line))

    def __len__(self) -> int:
        return len(self.rows)

    def __getitem__(self, idx: int) -> dict:
        return self.rows[idx]


def main() -> None:
    ap = argparse.ArgumentParser(
        description="GSPO training for activation-conditioned rewrites (full-model)"
    )
    ap.add_argument("--model", required=True, help="HF model ID to finetune")
    ap.add_argument(
        "--ref-model",
        default=None,
        help="Reference model for KL (default: same as --model, frozen)",
    )
    ap.add_argument(
        "--data", required=True, help="Path to JSONL generated by generate_rewrite_groups.py"
    )
    ap.add_argument("--output", required=True, help="Output directory for fine-tuned model")
    ap.add_argument("--batch-size", type=int, default=4)
    ap.add_argument("--epochs", type=int, default=1)
    ap.add_argument("--lr", type=float, default=1e-5)
    ap.add_argument("--clip-eps", type=float, default=0.2)
    ap.add_argument("--kl-coeff", type=float, default=0.05)
    ap.add_argument("--max-steps", type=int, default=0, help="Optional step limit")
    ap.add_argument("--grad-accum", type=int, default=1, help="Gradient accumulation steps")
    ap.add_argument("--fp16", action="store_true", help="Use AMP autocast for training")
    ap.add_argument(
        "--backend",
        choices=["torch", "mlx"],
        default="torch",
        help="Training backend (torch only supported)",
    )
    args = ap.parse_args()

    if args.backend != "torch":
        raise RuntimeError(
            "GSPO training currently supports backend='torch' only. MLX training not implemented yet."
        )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tok = AutoTokenizer.from_pretrained(args.model)
    model = AutoModelForCausalLM.from_pretrained(args.model)
    model.to(device)
    model.train()

    # Reference model for KL (frozen)
    ref_id = args.ref_model if args.ref_model else args.model
    ref_model = AutoModelForCausalLM.from_pretrained(ref_id)
    ref_model.to(device).eval()
    for p in ref_model.parameters():
        p.requires_grad_(False)

    ds = RewriteGroupDataset(Path(args.data))
    dl = DataLoader(ds, batch_size=args.batch_size, shuffle=True, drop_last=False)

    optim = torch.optim.AdamW(model.parameters(), lr=args.lr)
    scaler = torch.cuda.amp.GradScaler(enabled=args.fp16 and torch.cuda.is_available())

    step = 0
    for epoch in range(args.epochs):
        for bi, batch in enumerate(dl, start=1):
            prompts = batch["prompt"] if isinstance(batch, dict) else [x["prompt"] for x in batch]
            responses = (
                batch["response"] if isinstance(batch, dict) else [x["response"] for x in batch]
            )
            old_logp = (
                batch["old_logp"] if isinstance(batch, dict) else [x["old_logp"] for x in batch]
            )
            advantages = (
                batch["advantage"] if isinstance(batch, dict) else [x["advantage"] for x in batch]
            )

            if not isinstance(prompts, list):  # when DataLoader collates dict of lists
                prompts = [p for p in prompts]
                responses = [r for r in responses]
                old_logp = [float(x) for x in old_logp]
                advantages = [float(a) for a in advantages]

            old_logp_t = torch.tensor(old_logp, dtype=torch.float32, device=device)
            adv_t = torch.tensor(advantages, dtype=torch.float32, device=device)

            with torch.cuda.amp.autocast(enabled=args.fp16 and torch.cuda.is_available()):
                # Current and reference log-probs on responses
                logp_cur = batched_sequence_logprob(
                    model,
                    tok,
                    prompts,
                    responses,
                    device=device,
                    batch_size=max(1, args.batch_size // 2),
                    requires_grad=True,
                )
                with torch.no_grad():
                    logp_ref = batched_sequence_logprob(
                        ref_model,
                        tok,
                        prompts,
                        responses,
                        device=device,
                        batch_size=max(1, args.batch_size // 2),
                    )

                loss_pg = gspo_sequence_loss(
                    logp_cur=logp_cur,
                    logp_old=old_logp_t,
                    advantages=adv_t,
                    clip_epsilon=args.clip_eps,
                )
                kl = approx_sequence_kl(logp_cur=logp_cur, logp_ref=logp_ref)
                loss = loss_pg + args.kl_coeff * kl

            loss = loss / max(1, args.grad_accum)
            if scaler.is_enabled():
                scaler.scale(loss).backward()
            else:
                loss.backward()

            if bi % max(1, args.grad_accum) == 0:
                if scaler.is_enabled():
                    scaler.unscale_(optim)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    scaler.step(optim)
                    scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optim.step()
                optim.zero_grad(set_to_none=True)

            step += 1
            if step % 10 == 0:
                print(
                    f"step={step} loss={loss.item():.4f} pg={loss_pg.item():.4f} kl={kl.item():.4f}"
                )

            if args.max_steps and step >= args.max_steps:
                break
        if args.max_steps and step >= args.max_steps:
            break

    # Save fine-tuned model
    out_dir = Path(args.output)
    out_dir.mkdir(parents=True, exist_ok=True)
    model.save_pretrained(str(out_dir))
    tok.save_pretrained(str(out_dir))
    print(f"Saved fine-tuned model to {out_dir}")


if __name__ == "__main__":
    main()
